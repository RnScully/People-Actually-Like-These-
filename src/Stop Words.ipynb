{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:41:28.427746Z",
     "start_time": "2020-02-05T16:41:28.414643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:41:30.606697Z",
     "start_time": "2020-02-05T16:41:29.708569Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make Mongo stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:42:20.653437Z",
     "start_time": "2020-02-05T16:42:20.640332Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.reviews\n",
    "coll = db.collections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T21:09:44.294291Z",
     "start_time": "2020-01-31T21:09:43.506287Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# data_path ='/home/robert/nlp_case_study/wiki/*.txt'\n",
    "# paths = [file for file in iglob(os.path.expanduser(data_path))]\n",
    "# samples =random.choices(paths, k = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill mongodb from files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "data_path ='/home/robert/nlp_case_study/small/*.txt'\n",
    "paths = [file for file in iglob(os.path.expanduser(data_path))]\n",
    "\n",
    "for path in samples:\n",
    "    f = open(path, 'rt')\n",
    "    raw = f.read()\n",
    "    coll.insert_one({'text': raw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:42:22.598368Z",
     "start_time": "2020-02-05T16:42:22.579642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = coll.find({})\n",
    "coll.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in docuents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:07:53.656663Z",
     "start_time": "2020-02-04T20:07:53.540340Z"
    }
   },
   "outputs": [],
   "source": [
    "#documents = [article['text'].lower() for article in coll.find()]\n",
    "\n",
    "#documents = coll.aggregate([{ $sample: { size: 5000 } }])\n",
    "\n",
    "documents = np.load('data/english_arr.npy', allow_pickle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:09:06.216801Z",
     "start_time": "2020-02-04T20:09:06.210164Z"
    }
   },
   "outputs": [],
   "source": [
    "words = documents[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision point: it takes 2 seconds to lemmatize per document, on this machine that would require 11 days of runtime for 316 thousand documents, Not worthwhile for a single day project, stemming would take 170 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:15:57.570903Z",
     "start_time": "2020-02-04T20:15:57.563289Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "my_words = set([ \n",
    "                \n",
    "               ])\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_words)\n",
    "#topics[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "Decision Point: \n",
    "- stemming or lemmizing the full dataset requires more runtime cycles than we have available. \n",
    "- lemmaing: 2 seconds per document\n",
    "- stemming: .024 secs per document\n",
    "Decision: small sample set was linear from topics in the lower numbers, bigger pipe needs a rando sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:11:46.389639Z",
     "start_time": "2020-02-04T20:10:04.684334Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmed_documents = []\n",
    "wordnet = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#sample = random.sample(documents, k=15000)\n",
    "''' samples out a small section so we can run quickly'''\n",
    "for doc in words:\n",
    "      ### hazard, make sure ' ' not ''\n",
    "    a = ' '.join([porter.stem(word) for word in doc.split()])\n",
    "    stemmed_documents.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:15:18.240313Z",
     "start_time": "2020-02-04T20:15:18.232625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'live love laugh a lot is a special book interest charact make you laugh- you must take a look voic their opinion in humor form move line by line entertain to such an extent- you must take your time lov the way jacquelyn speak through sli fox ov and over you have to laugh as he come out of hi box veri sli he tri to be from behind hi cloth excit on each page you must laugh a lot laught ring out as ingrid doe her stuff a woman who could final say, “enough is enough” us me lord jacquelyn say on page fifty-eight god’ bless come in mani way jacquelyn point out to us honesti and humor you will find within these page so much a great read and must for everi home l love laugh a lotoh how humorousthi book is written by jacquelyn sturg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:19:02.495829Z",
     "start_time": "2020-02-04T20:19:02.438920Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wordnet = WordNetLemmatizer()\n",
    "# porter = PorterStemmer()\n",
    "# snowball = SnowballStemmer('english')\n",
    "# #[wordnet.lemmatize(word) for word in documents[2].split()]\n",
    "# a = ''.join([snowball.stem(word) for word in documents[50].split()])\n",
    "# a\n",
    "\n",
    "\n",
    "stemmed2 = []\n",
    "for i in words:\n",
    "    stemmed2.append(i.replace('.', ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:19:09.771412Z",
     "start_time": "2020-02-04T20:19:05.239818Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tfidfvect = TfidfVectorizer(stop_words=my_stop_words)\n",
    "#sample = random.sample(documents, k=20000)\n",
    "tfidf_vectorized = tfidfvect.fit_transform(stemmed2)\n",
    "X = tfidf_vectorized\n",
    "features = tfidfvect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:20:17.570504Z",
     "start_time": "2020-02-04T20:20:11.346960Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = NMF(n_components=5, max_iter=100)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "\n",
    "#find laten topics\n",
    "topics = []\n",
    "for row in H:\n",
    "    topics.append([features[items]for items in row.argsort(axis=0)[-40:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Topic zero is Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:20:19.159343Z",
     "start_time": "2020-02-04T20:20:19.150282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does',\n",
       " 'thing',\n",
       " 'hard',\n",
       " 'man',\n",
       " 'pages',\n",
       " 'want',\n",
       " 'main',\n",
       " 'world',\n",
       " 'say',\n",
       " 'good',\n",
       " 'lot',\n",
       " 'make',\n",
       " 'going',\n",
       " 'author',\n",
       " 'thought',\n",
       " 'little',\n",
       " 'writing',\n",
       " 'end',\n",
       " 'love',\n",
       " 'felt',\n",
       " 'time',\n",
       " 'feel',\n",
       " 'things',\n",
       " 'novel',\n",
       " 'family',\n",
       " 'don',\n",
       " 'character',\n",
       " 'think',\n",
       " 'did',\n",
       " 'know',\n",
       " 'way',\n",
       " 'characters',\n",
       " 'didn',\n",
       " 'people',\n",
       " 'life',\n",
       " 'story',\n",
       " 'really',\n",
       " 'just',\n",
       " 'like',\n",
       " 'book']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film and Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:20:43.714626Z",
     "start_time": "2020-02-04T20:20:43.705881Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ended',\n",
       " 'slow',\n",
       " 'fascinating',\n",
       " 'couldn',\n",
       " 'highly',\n",
       " 'did',\n",
       " 'started',\n",
       " 'forward',\n",
       " 'thought',\n",
       " 'lot',\n",
       " 'amazing',\n",
       " 'disturbing',\n",
       " 'style',\n",
       " 'finish',\n",
       " 'depressing',\n",
       " 'finished',\n",
       " 'wonderful',\n",
       " 'funny',\n",
       " 'stars',\n",
       " 'enjoy',\n",
       " 'surprised',\n",
       " 'author',\n",
       " 'parts',\n",
       " 'overall',\n",
       " 'club',\n",
       " 'story',\n",
       " 'definitely',\n",
       " 'characters',\n",
       " 'worth',\n",
       " 'sad',\n",
       " 'interesting',\n",
       " 'movie',\n",
       " 'recommend',\n",
       " 'thoroughly',\n",
       " 'beautifully',\n",
       " 'really',\n",
       " 'book',\n",
       " 'reading',\n",
       " 'written',\n",
       " 'enjoyed']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:21:02.532348Z",
     "start_time": "2020-02-04T20:21:02.518088Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classic',\n",
       " 'liked',\n",
       " 'fun',\n",
       " 'list',\n",
       " 'couldn',\n",
       " 'want',\n",
       " 'interesting',\n",
       " 'easy',\n",
       " 'high',\n",
       " 'quick',\n",
       " 'took',\n",
       " 'second',\n",
       " 'funny',\n",
       " 'glad',\n",
       " 'definitely',\n",
       " 'just',\n",
       " 'oprah',\n",
       " 'better',\n",
       " 'school',\n",
       " 'club',\n",
       " 'amazing',\n",
       " 'recommend',\n",
       " 'think',\n",
       " 'bryson',\n",
       " 'long',\n",
       " 'remember',\n",
       " 'times',\n",
       " 'movie',\n",
       " 'favorite',\n",
       " 'good',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'great',\n",
       " 'best',\n",
       " 'time',\n",
       " 've',\n",
       " 'loved',\n",
       " 'books',\n",
       " 'book',\n",
       " 'read']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:21:43.862280Z",
     "start_time": "2020-02-04T20:21:43.854819Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epic',\n",
       " 'female',\n",
       " 'world',\n",
       " 'loved',\n",
       " 'action',\n",
       " 'wwii',\n",
       " 'novels',\n",
       " 'england',\n",
       " 'suspense',\n",
       " 'medieval',\n",
       " 'twists',\n",
       " 'set',\n",
       " 'author',\n",
       " 'women',\n",
       " 'page',\n",
       " 'building',\n",
       " 'century',\n",
       " 'writing',\n",
       " 'kingsbridge',\n",
       " 'end',\n",
       " 'history',\n",
       " 'spy',\n",
       " 'war',\n",
       " 'follet',\n",
       " 'thriller',\n",
       " 'love',\n",
       " 'cathedral',\n",
       " 'plot',\n",
       " 'interesting',\n",
       " 'novel',\n",
       " 'fiction',\n",
       " 'earth',\n",
       " 'pillars',\n",
       " 'historical',\n",
       " 'good',\n",
       " 'great',\n",
       " 'characters',\n",
       " 'story',\n",
       " 'ken',\n",
       " 'follett']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:22:12.204966Z",
     "start_time": "2020-02-04T20:22:12.193721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['susannah',\n",
       " 'quest',\n",
       " 'fellowship',\n",
       " 'best',\n",
       " 'sam',\n",
       " 'review',\n",
       " 'trilogy',\n",
       " 'lotr',\n",
       " 'ka',\n",
       " 'say',\n",
       " 'jake',\n",
       " 'middle',\n",
       " 'epic',\n",
       " 'movies',\n",
       " 'journey',\n",
       " 'hobbit',\n",
       " 'earth',\n",
       " 'like',\n",
       " 'just',\n",
       " 'reading',\n",
       " 'stephen',\n",
       " 'potter',\n",
       " 'frodo',\n",
       " 'ring',\n",
       " 'gunslinger',\n",
       " 'harry',\n",
       " 'rings',\n",
       " 'time',\n",
       " 'lord',\n",
       " 'love',\n",
       " 'favorite',\n",
       " 'fantasy',\n",
       " 'world',\n",
       " 'dark',\n",
       " 'roland',\n",
       " 'tower',\n",
       " 'tolkien',\n",
       " 'king',\n",
       " 'books',\n",
       " 'series']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 5 appears to be math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T20:22:32.188335Z",
     "start_time": "2020-02-04T20:22:32.169364Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-45e7c5f424cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "topics[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T22:08:06.890822Z",
     "start_time": "2020-01-31T22:08:06.882098Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['musical',\n",
       " 'recording',\n",
       " 'film',\n",
       " 'charts',\n",
       " 'cd',\n",
       " 'override',\n",
       " 'studio',\n",
       " 'single',\n",
       " 'recorded',\n",
       " 'track',\n",
       " 'live',\n",
       " 'bass',\n",
       " 'copyright',\n",
       " 'singles',\n",
       " 'yes',\n",
       " 'low_resolution',\n",
       " 'licensing',\n",
       " 'other_information',\n",
       " 'replaceability',\n",
       " 'non',\n",
       " 'chart',\n",
       " 'songs',\n",
       " 'use',\n",
       " 'vocals',\n",
       " 'billboard',\n",
       " 'free',\n",
       " 'label',\n",
       " 'allmusic',\n",
       " 'released',\n",
       " 'guitar',\n",
       " 'rock',\n",
       " 'records',\n",
       " 'song',\n",
       " 'rationale',\n",
       " 'cover',\n",
       " 'artist',\n",
       " 'albums',\n",
       " 'music',\n",
       " 'band',\n",
       " 'album']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def hand_label_topics(H, features):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic, and prompt the user\n",
    "    to label each topic. The user should use their humanness to figure out what\n",
    "    each latent topic is capturing.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_five = np.argsort(row)[::-1][:20]\n",
    "        print('topic', i)\n",
    "        print('-->', ' '.join(vocabulary[top_five]))\n",
    "        label = input('please label this topic: ')\n",
    "        hand_labels.append(label)\n",
    "        print()\n",
    "    return hand_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### dict of own lemmaing\n",
    "\n",
    "- 10, 65, 18 to date implication?\n",
    "\n",
    "- 2004, 2010, 2016, 2010, 2009, 2007, 2015, 2013, to date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
