{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T17:54:22.524789Z",
     "start_time": "2020-02-06T17:54:21.509738Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, SCORERS\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tools import save_model, predict_one, predict_many, report_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     5,
     15
    ]
   },
   "outputs": [],
   "source": [
    "def save_model(model,name):\n",
    "    file_ext= '.sav'\n",
    "    path = 'models/'\n",
    "    pickle.dump(model, open(path+name+file_ext, 'wb'))\n",
    "    \n",
    "def predict_one(string, model_name, vectorizor_name):\n",
    "    \n",
    "    path = 'models/'\n",
    "        \n",
    "    tfid = pickle.load(open(path+vectorizor_name, 'rb'))\n",
    "    tfidfed = tfid.transform([string])\n",
    "\n",
    "    model = pickle.load(open(path+model_name, 'rb'))\n",
    "    return model.predict(tfidfed)\n",
    "\n",
    "def predict_many(review_list, model_name, vectorizor_name):\n",
    "    path = 'models/'\n",
    "        \n",
    "    tfid = pickle.load(open(path+vectorizor_name, 'rb'))\n",
    "    tfidfed = tfid.transform(review_list)\n",
    "\n",
    "    model = pickle.load(open(path+model_name, 'rb'))\n",
    "    return model.predict(tfidfed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load things stored from other pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T19:10:15.781253Z",
     "start_time": "2020-02-06T19:10:15.652347Z"
    }
   },
   "outputs": [],
   "source": [
    "arr  = np.load('data/english_arr.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T17:54:25.918597Z",
     "start_time": "2020-02-06T17:54:25.249585Z"
    }
   },
   "outputs": [],
   "source": [
    "ngr  = np.load('data/2grams.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize and Stem, or: Pre-handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T19:10:17.736456Z",
     "start_time": "2020-02-06T19:10:17.729258Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = arr[:,3]\n",
    "ratings = arr[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T19:10:19.639150Z",
     "start_time": "2020-02-06T19:10:19.606243Z"
    }
   },
   "outputs": [],
   "source": [
    "# good vs bad rating 4,5 will be good (1), -1, 1,2,3 will be bad (0)\n",
    "\n",
    "y = ratings\n",
    "gvb = []\n",
    "\n",
    "for i in y:\n",
    "    if i <4:\n",
    "        gvb.append(0)\n",
    "    else:\n",
    "        gvb.append(1)\n",
    "        \n",
    "\n",
    "np.save('gvbtrain.npy', gvb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T19:09:56.530131Z",
     "start_time": "2020-02-06T19:09:56.524334Z"
    }
   },
   "outputs": [],
   "source": [
    "test_y = [1,2,3,4,5,5,4,3,2]\n",
    "gvbt = []\n",
    "\n",
    "for i in test_y:\n",
    "    if i < 4:\n",
    "        gvbt.append(0)\n",
    "    else:\n",
    "        gvbt.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T19:09:58.108316Z",
     "start_time": "2020-02-06T19:09:58.103239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gvbt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Selections\n",
    "gvb is a training set that will let the model predict good/bad valences, if a rating is 4 or 5, gvb = 1\n",
    "\n",
    "FourFive is a sub-selection of the whole dataset with just the 4 and five ratings, to try and train a this is best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid = TfidfVectorizer(stop_words ='english', lowercase = False, max_features = 5000)\n",
    "tfidfed = tfid.fit_transform(lang)\n",
    "\n",
    "X = tfidfed\n",
    "y = ratings.reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, gvb, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T17:54:42.782047Z",
     "start_time": "2020-02-06T17:54:37.874960Z"
    }
   },
   "outputs": [],
   "source": [
    "#sngr = [' '.join(item) for item in ngr] # <- Run only once each notebook\n",
    "\n",
    "\n",
    "ngramTFV5k = TfidfVectorizer(lowercase = False, max_features = 5000)\n",
    "ngramX = ngramTFV5k.fit_transform(sngr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T17:55:13.655597Z",
     "start_time": "2020-02-06T17:55:13.552550Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('ngramTF5kneg', ngramTFV5k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T17:56:23.674008Z",
     "start_time": "2020-02-06T17:56:23.663099Z"
    }
   },
   "outputs": [],
   "source": [
    "gvb  = np.load('gvbtrain.npy', allow_pickle = True)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(ngramX, gvb, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6763739203546587"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "knn_class = KNeighborsClassifier(n_neighbors = 18, n_jobs =-1)\n",
    "knn_class.fit(X_train, y_train)\n",
    "knn_class.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An early random forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=True, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest1 = RandomForestClassifier(min_samples_split = 10, oob_score=True, n_jobs =-1, n_estimators = 100)\n",
    "forest1.fit(X, gvb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8460296640096862"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest1.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(tfid, 'tf84')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest with more trees, mowed down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=True, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manysaplings = RandomForestClassifier(min_samples_split = 10, oob_score=True, n_jobs =-1, n_estimators = 1000)\n",
    "manysaplings.fit(X, gvb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8533699929371406"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manysaplings.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest on Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T19:16:00.424712Z",
     "start_time": "2020-02-06T19:11:38.678818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=True, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ngramforest_forest = RandomForestClassifier(min_samples_split = 10, oob_score=True, n_jobs =-1, n_estimators = 1000)\n",
    "ngramforest_forest.fit(ngramX, gvb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T19:18:19.207587Z",
     "start_time": "2020-02-06T19:18:16.930357Z"
    }
   },
   "outputs": [],
   "source": [
    "save_model(ngramforest_forest, 'ngramRF85')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T19:17:03.168523Z",
     "start_time": "2020-02-06T19:17:03.162504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8509736656240541"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramforest_forest.oob_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training score: 0.9978916456458717, Testing score: 0.8330658105939005\n",
    "\n",
    "- tn       fp   fn   tp\n",
    "- 8072 280 1904 2827\n",
    "\n",
    "precision: 0.9098809140650145 recall: 0.5975480870851828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9978916456458717, Testing score: 0.8330658105939005\n",
      "tn   fp   fn   tp\n",
      "8072 280 1904 2827\n",
      "precision: 0.9098809140650145 recall: 0.5975480870851828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9978916456458717,\n",
       " 0.8330658105939005,\n",
       " 0.9098809140650145,\n",
       " 0.5975480870851828]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_score(ngramforest_fortts, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with vaders appended to vectorized space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaders_arr = np.load('data/training_vaders.npy', allow_pickle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramTFV10k = TfidfVectorizer(lowercase = False, max_features = 10000)\n",
    "ngramX = ngramTFV10k.fit_transform(sngr)\n",
    "vadersX = hstack([vaders_arr, ngramX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=True, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaders_forest = RandomForestClassifier(min_samples_split = 10, oob_score=True, n_jobs =-1, n_estimators = 1000)\n",
    "vaders_forest.fit(vadersX, gvb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vaders_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-96921cc61e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvadersX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgvb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvaders_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moob_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vaders_forest' is not defined"
     ]
    }
   ],
   "source": [
    "gvb  = np.load('gvbtrain.npy', allow_pickle = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vadersX, gvb, test_size=0.33)\n",
    "vaders_forest.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def report_score(model, X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    a function that reports the accuracy of the model.\n",
    "    Attributes:\n",
    "    models (lst): a list of instansiated models to test\n",
    "    Returns:\n",
    "    out array, model name, training score, testing score, precision, recall\n",
    "    '''\n",
    "    training_score = model.score(X_train, y_train)\n",
    "    testing_score = model.score(X_test, y_test)\n",
    "    print('Training score: {}, Testing score: {}'.format(training_score, testing_score))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,model.predict(X_test)).ravel()\n",
    "    precision = tp/(fp+tp)\n",
    "    recall = tp/(fn+tp)\n",
    "    print('tn', '  fp', '  fn', '  tp')\n",
    "    print(tn, fp, fn, tp)\n",
    "    print('precision: '+str(precision), 'recall: '+ str(recall))\n",
    "    out_lst = [training_score, testing_score, precision, recall]\n",
    "    return out_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9983057866797184, Testing score: 0.8350531223725445\n",
      "tn   fp   fn   tp\n",
      "8197 263 1895 2728\n",
      "precision: 0.912069541959211 recall: 0.590093013194895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9983057866797184, 0.8350531223725445, 0.912069541959211, 0.590093013194895]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaders_forest_tts = RandomForestClassifier(min_samples_split = 10, oob_score=True, n_jobs =-1, n_estimators = 1000)\n",
    "vaders_forest_tts.fit(X_train, y_train)\n",
    "report_score(vaders_forest_tts, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25754"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gvb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, gvb, test_size=0.33)\n",
    "\n",
    "grad_boost = GradientBoostingClassifier()\n",
    "grad_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_boost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5099cf5182a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreport_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_boost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad_boost' is not defined"
     ]
    }
   ],
   "source": [
    "report_score(grad_boost, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient boosting with the n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ngramX, gvb, test_size=0.33)\n",
    "\n",
    "ngram_gboost = GradientBoostingClassifier(max_features = )\n",
    "ngram_gboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.7490681826738451, Testing score: 0.7307956890621418\n",
      "tn   fp   fn   tp\n",
      "8040 410 3112 1521\n",
      "precision: 0.787674779906784 recall: 0.3282969997841571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7490681826738451, 0.7307956890621418, 0.787674779906784, 0.3282969997841571]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_score(ngram_gboost, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch on ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_grid = {'max_depth': [10, 20, None],\n",
    "#                       'max_features': [None],\n",
    "#                       #'min_samples_split': [],\n",
    "#                       'min_samples_leaf': [50,200],\n",
    "#                       'n_estimators':[200, 250, 300]}                        \n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "# gbr_gridsearch = GridSearchCV(GradientBoostingClassifier(),\n",
    "#                              ngram_grid,\n",
    "#                              n_jobs=-1,\n",
    "#                              verbose=True,\n",
    "#                              scoring='f1')\n",
    "# gbr_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=50, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_gridsearch.best_estimator_\n",
    "#gbr_gridsearch.best_score_\n",
    "#gbr_gridsearch.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_is_fitted',\n",
       " '_estimator_type',\n",
       " '_format_results',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_required_parameters',\n",
       " '_run_search',\n",
       " 'best_estimator_',\n",
       " 'best_index_',\n",
       " 'best_params_',\n",
       " 'best_score_',\n",
       " 'classes_',\n",
       " 'cv',\n",
       " 'cv_results_',\n",
       " 'decision_function',\n",
       " 'error_score',\n",
       " 'estimator',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'iid',\n",
       " 'inverse_transform',\n",
       " 'multimetric_',\n",
       " 'n_jobs',\n",
       " 'n_splits_',\n",
       " 'param_grid',\n",
       " 'pre_dispatch',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'refit',\n",
       " 'refit_time_',\n",
       " 'return_train_score',\n",
       " 'score',\n",
       " 'scorer_',\n",
       " 'scoring',\n",
       " 'set_params',\n",
       " 'transform',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(gbr_gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.8228605850683333, Testing score: 0.7827715355805244\n",
      "tn   fp   fn   tp\n",
      "7514 838 2004 2727\n",
      "precision: 0.7649368863955119 recall: 0.5764109067850349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8228605850683333,\n",
       " 0.7827715355805244,\n",
       " 0.7649368863955119,\n",
       " 0.5764109067850349]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_score(logreg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heey...that's...pretty good. lets investigate what logreg thinks is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_predict_proba_lr',\n",
       " 'class_weight',\n",
       " 'classes_',\n",
       " 'coef_',\n",
       " 'decision_function',\n",
       " 'densify',\n",
       " 'dual',\n",
       " 'fit',\n",
       " 'fit_intercept',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'intercept_scaling',\n",
       " 'l1_ratio',\n",
       " 'max_iter',\n",
       " 'multi_class',\n",
       " 'n_iter_',\n",
       " 'n_jobs',\n",
       " 'penalty',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'random_state',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'solver',\n",
       " 'sparsify',\n",
       " 'tol',\n",
       " 'verbose',\n",
       " 'warm_start']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_coef =logreg.coef_\n",
    "dir(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " 'class_weight',\n",
       " 'dual',\n",
       " 'fit_intercept',\n",
       " 'intercept_scaling',\n",
       " 'l1_ratio',\n",
       " 'max_iter',\n",
       " 'multi_class',\n",
       " 'n_jobs',\n",
       " 'penalty',\n",
       " 'random_state',\n",
       " 'solver',\n",
       " 'tol',\n",
       " 'verbose',\n",
       " 'warm_start']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg._get_param_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'rf84.sav' is an 84% accurate random forest model. \n",
    "- 'tf84.sav' is the vectorizer formulated to work for the 84% rf, with 5000 rows. Can fit into other things with 500 features. \n",
    "- 'ngramRF85.sav' is the ngramed random forest, 85% r1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"i found this book to be atrocious\"\n",
    "\n",
    "predict_one(string, 'ngramRF85.sav', 'tf84.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_one(string, 'rf84.sav', 'tf84.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_many"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv4689841a946143dd80c9fcc86c644564"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
