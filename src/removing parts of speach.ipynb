{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:57:21.351042Z",
     "start_time": "2020-02-05T16:57:21.344271Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from tools import update_progress \n",
    "\n",
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:05:21.532376Z",
     "start_time": "2020-02-05T17:05:21.512956Z"
    }
   },
   "outputs": [],
   "source": [
    "gvb = np.load('gvbtrain.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:40:38.288639Z",
     "start_time": "2020-02-05T14:40:38.283506Z"
    }
   },
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:23:29.020794Z",
     "start_time": "2020-02-05T14:23:23.983914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n"
     ]
    }
   ],
   "source": [
    "l = [1,2,3,4,5]\n",
    "for idx, i in enumerate(l):\n",
    "    time.sleep(1)\n",
    "    progress_bar = idx\n",
    "    update_progress((idx+1)/len(l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:31:32.556291Z",
     "start_time": "2020-02-05T14:31:32.541058Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#needs work\n",
    "def report_score(models, X_test, y_test):\n",
    "    '''\n",
    "    a function that reports the accuracy of the model.\n",
    "    Attributes:\n",
    "    models (lst): a list of instansiated models to test\n",
    "    Returns:\n",
    "    out array, model name, training score, testing score, precision, recall\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tts(X, y)\n",
    "    out = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        training_score = rf.score(X_train, y_train)\n",
    "        testing_score = rf.score(X_test, y_test)\n",
    "        print('Training score: {}, Testing score: {}'.format(training_score, testing_score))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test,rf.predict(X_test)).ravel()\n",
    "        precision = tp/(fp+tp)\n",
    "        recall = tp/(fn+tp)\n",
    "        print('tn', '  fp', '  fn', '  tp')\n",
    "        print(tn, fp, fn, tp)\n",
    "        print('precision: '+str(precision), 'recall: '+ str(recall))\n",
    "        out_lst = [model,training_score, testing_score, precision, recall]\n",
    "        out.append(out_lst)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:34:08.781080Z",
     "start_time": "2020-02-05T14:34:08.660665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " c_g_rev_cleaner.py     models\t\t        samples.py\r\n",
      "'Cleaning Pipe.ipynb'   Models.ipynb\t        samples.txt\r\n",
      " data\t\t       'NLP experiment.ipynb'  'Stop Words.ipynb'\r\n",
      " EDA.ipynb\t        predictor.py\t        tools.py\r\n",
      " geckodriver.log        progress.txt\t        Untitled.ipynb\r\n",
      " g_rev_scrape.py        __pycache__\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:35:55.132722Z",
     "start_time": "2020-02-05T14:35:55.013002Z"
    }
   },
   "outputs": [],
   "source": [
    "english_arr = np.load('data/english_arr.npy',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:36:55.015818Z",
     "start_time": "2020-02-05T14:36:55.010538Z"
    }
   },
   "outputs": [],
   "source": [
    "words = english_arr[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:30:03.760033Z",
     "start_time": "2020-02-05T16:29:50.783Z"
    }
   },
   "outputs": [],
   "source": [
    "text = word_tokenize(words[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:25:13.483598Z",
     "start_time": "2020-02-05T16:25:13.407986Z"
    }
   },
   "outputs": [],
   "source": [
    "j = nltk.pos_tag(text)\n",
    "\n",
    "remove_parts = set(['PRP','PRP$','NNP','NNPS','EX','PDT','UH','WP','WPS','WRB'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:31:16.763492Z",
     "start_time": "2020-02-05T16:30:18.323454Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(items) for items in words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:52:29.889530Z",
     "start_time": "2020-02-05T16:52:29.860566Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def remove_parts_of_speech(tokenized_corpus, parts_to_remove):\n",
    "    '''function which uses nltk position tagging to remove parts of speach\n",
    "    ++++++++++\n",
    "    Attributes\n",
    "    tokenized_corpus (lst): a list of lists: the corpus of documents, each doc transformed into a list of tokens\n",
    "    parts_to_remove (lst): a list of the NLTK parts of speach that you want removed\n",
    "    \n",
    "    Returns: \n",
    "    ++++++++++\n",
    "    no_pronouns(lst): a list of lists simmilar to tokenized_corpus containing none of the parts of speech you wanted removed\n",
    "    '''\n",
    "    remove = set(parts_to_remove)\n",
    "    no_pronouns = []\n",
    "    for text in tokenized_corpus:\n",
    "        j =nltk.pos_tag(text)\n",
    "        review = []\n",
    "        for pos in j:\n",
    "            if pos[1] in remove:\n",
    "                continue\n",
    "            else:\n",
    "                review.append(pos[0])\n",
    "        ' '.join(review)\n",
    "        no_pronouns.append(review)\n",
    "    \n",
    "    return no_pronouns_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:50:08.756495Z",
     "start_time": "2020-02-05T16:45:02.777533Z"
    }
   },
   "outputs": [],
   "source": [
    "without_pronouns_words = remove_parts_of_speech(tokenized_corpus, remove_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:23:09.666560Z",
     "start_time": "2020-02-05T16:23:09.570468Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', ','), 381867),\n",
       " (('the', 'DT'), 361935),\n",
       " (('.', '.'), 344774),\n",
       " (('and', 'CC'), 226619),\n",
       " (('of', 'IN'), 211056),\n",
       " (('to', 'TO'), 192696),\n",
       " (('a', 'DT'), 186677),\n",
       " (('is', 'VBZ'), 118230),\n",
       " (('in', 'IN'), 115326),\n",
       " (('was', 'VBD'), 77819),\n",
       " (('this', 'DT'), 74922),\n",
       " (('book', 'NN'), 72787),\n",
       " (('that', 'IN'), 58600),\n",
       " (('for', 'IN'), 56996),\n",
       " (('with', 'IN'), 52984),\n",
       " (('but', 'CC'), 47099),\n",
       " (('The', 'DT'), 41939),\n",
       " (('as', 'IN'), 40057),\n",
       " (('not', 'RB'), 39439),\n",
       " ((\"n't\", 'RB'), 36967),\n",
       " (('on', 'IN'), 36914),\n",
       " (('are', 'VBP'), 34423),\n",
       " (('be', 'VB'), 32801),\n",
       " ((\"'s\", 'POS'), 32310),\n",
       " ((')', ')'), 30817),\n",
       " (('about', 'IN'), 30164),\n",
       " (('(', '('), 29289),\n",
       " (('at', 'IN'), 28440),\n",
       " ((\"'s\", 'VBZ'), 26897),\n",
       " (('story', 'NN'), 25775),\n",
       " (('so', 'RB'), 25733),\n",
       " (('one', 'CD'), 25683),\n",
       " (('that', 'WDT'), 25642),\n",
       " (('from', 'IN'), 25159),\n",
       " (('an', 'DT'), 24927),\n",
       " (('by', 'IN'), 24284),\n",
       " (('!', '.'), 24175),\n",
       " (('or', 'CC'), 23165),\n",
       " (('``', '``'), 21865),\n",
       " (('all', 'DT'), 21601),\n",
       " (('just', 'RB'), 21450),\n",
       " (('?', '.'), 21107),\n",
       " (('like', 'IN'), 20909),\n",
       " (('have', 'VBP'), 20081),\n",
       " ((\"''\", \"''\"), 19517),\n",
       " (('time', 'NN'), 18986),\n",
       " (('had', 'VBD'), 18820),\n",
       " (('has', 'VBZ'), 18539),\n",
       " (('would', 'MD'), 18473),\n",
       " (('that', 'DT'), 18448),\n",
       " ((':', ':'), 18004),\n",
       " (('can', 'MD'), 17843),\n",
       " (('This', 'DT'), 17696),\n",
       " (('did', 'VBD'), 17356),\n",
       " (('characters', 'NNS'), 16934),\n",
       " (('really', 'RB'), 15923),\n",
       " (('some', 'DT'), 15308),\n",
       " (('were', 'VBD'), 15211),\n",
       " (('because', 'IN'), 15194),\n",
       " (('life', 'NN'), 15096),\n",
       " (('have', 'VB'), 14960),\n",
       " (('which', 'WDT'), 14751),\n",
       " (('up', 'RP'), 14728),\n",
       " (('read', 'VB'), 14659),\n",
       " (('very', 'RB'), 14454),\n",
       " (('if', 'IN'), 14118),\n",
       " (('into', 'IN'), 13866),\n",
       " (('...', ':'), 13861),\n",
       " (('do', 'VBP'), 13326),\n",
       " (('books', 'NNS'), 13265),\n",
       " (('could', 'MD'), 13109),\n",
       " ((';', ':'), 13071),\n",
       " (('people', 'NNS'), 12963),\n",
       " (('-', ':'), 12862),\n",
       " (('way', 'NN'), 12787),\n",
       " (('good', 'JJ'), 12532),\n",
       " (('will', 'MD'), 12417),\n",
       " (('even', 'RB'), 12365),\n",
       " (('than', 'IN'), 12277),\n",
       " (('out', 'RP'), 11210),\n",
       " (('many', 'JJ'), 11164),\n",
       " (('been', 'VBN'), 11131),\n",
       " (('other', 'JJ'), 11105),\n",
       " (('through', 'IN'), 11065),\n",
       " (('novel', 'NN'), 10880),\n",
       " (('first', 'JJ'), 10748),\n",
       " (('more', 'RBR'), 10710),\n",
       " (('no', 'DT'), 10522),\n",
       " (('reading', 'VBG'), 10447),\n",
       " (('much', 'JJ'), 10445),\n",
       " (('And', 'CC'), 10375),\n",
       " (('more', 'JJR'), 10326),\n",
       " (('also', 'RB'), 10219),\n",
       " (('But', 'CC'), 10199),\n",
       " (('does', 'VBZ'), 9674),\n",
       " (('character', 'NN'), 9480),\n",
       " (('world', 'NN'), 9461),\n",
       " ((\"'m\", 'VBP'), 9300),\n",
       " (('too', 'RB'), 9271),\n",
       " (('family', 'NN'), 9035)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.most_common(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:53:43.884705Z",
     "start_time": "2020-02-05T16:53:43.652220Z"
    }
   },
   "outputs": [],
   "source": [
    "str_without_pronouns =[' '.join(i) for i in without_pronouns_words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:54:07.181501Z",
     "start_time": "2020-02-05T16:54:07.170334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Complex book about the struggle between the 's of , , and the 's totalitarian leader , `` the , '' against the dry and barren beauty of the desert landscape . A fantasy which includes special powers . This is part of The of series , which have n't read or been aware of .\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:07:27.074097Z",
     "start_time": "2020-02-05T17:07:22.611653Z"
    }
   },
   "outputs": [],
   "source": [
    "tfid = TfidfVectorizer(stop_words ='english', lowercase = True, max_features = 5000)\n",
    "tfidfed = tfid.fit_transform(str_without_pronouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:07:49.421869Z",
     "start_time": "2020-02-05T17:07:27.076071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=True, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidfed\n",
    "\n",
    "forest1 = RandomForestClassifier(min_samples_split = 10, oob_score=True, n_jobs =-1, n_estimators = 100)\n",
    "forest1.fit(X, gvb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:07:52.320049Z",
     "start_time": "2020-02-05T17:07:52.306803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8433306427202099"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest1.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:09:11.595300Z",
     "start_time": "2020-02-05T17:09:11.573581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39644"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv4689841a946143dd80c9fcc86c644564"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
