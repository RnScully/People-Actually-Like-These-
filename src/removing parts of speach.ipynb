{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:42:03.031433Z",
     "start_time": "2020-02-05T16:42:03.027225Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from tools import update_progress \n",
    "\n",
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:40:38.288639Z",
     "start_time": "2020-02-05T14:40:38.283506Z"
    }
   },
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:23:29.020794Z",
     "start_time": "2020-02-05T14:23:23.983914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n"
     ]
    }
   ],
   "source": [
    "l = [1,2,3,4,5]\n",
    "for idx, i in enumerate(l):\n",
    "    time.sleep(1)\n",
    "    progress_bar = idx\n",
    "    update_progress((idx+1)/len(l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:31:32.556291Z",
     "start_time": "2020-02-05T14:31:32.541058Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#needs work\n",
    "def report_score(models, X_test, y_test):\n",
    "    '''\n",
    "    a function that reports the accuracy of the model.\n",
    "    Attributes:\n",
    "    models (lst): a list of instansiated models to test\n",
    "    Returns:\n",
    "    out array, model name, training score, testing score, precision, recall\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tts(X, y)\n",
    "    out = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        training_score = rf.score(X_train, y_train)\n",
    "        testing_score = rf.score(X_test, y_test)\n",
    "        print('Training score: {}, Testing score: {}'.format(training_score, testing_score))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test,rf.predict(X_test)).ravel()\n",
    "        precision = tp/(fp+tp)\n",
    "        recall = tp/(fn+tp)\n",
    "        print('tn', '  fp', '  fn', '  tp')\n",
    "        print(tn, fp, fn, tp)\n",
    "        print('precision: '+str(precision), 'recall: '+ str(recall))\n",
    "        out_lst = [model,training_score, testing_score, precision, recall]\n",
    "        out.append(out_lst)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:34:08.781080Z",
     "start_time": "2020-02-05T14:34:08.660665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " c_g_rev_cleaner.py     models\t\t        samples.py\r\n",
      "'Cleaning Pipe.ipynb'   Models.ipynb\t        samples.txt\r\n",
      " data\t\t       'NLP experiment.ipynb'  'Stop Words.ipynb'\r\n",
      " EDA.ipynb\t        predictor.py\t        tools.py\r\n",
      " geckodriver.log        progress.txt\t        Untitled.ipynb\r\n",
      " g_rev_scrape.py        __pycache__\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:35:55.132722Z",
     "start_time": "2020-02-05T14:35:55.013002Z"
    }
   },
   "outputs": [],
   "source": [
    "english_arr = np.load('data/english_arr.npy',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:36:55.015818Z",
     "start_time": "2020-02-05T14:36:55.010538Z"
    }
   },
   "outputs": [],
   "source": [
    "words = english_arr[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:30:03.760033Z",
     "start_time": "2020-02-05T16:29:50.783Z"
    }
   },
   "outputs": [],
   "source": [
    "text = word_tokenize(words[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:25:13.483598Z",
     "start_time": "2020-02-05T16:25:13.407986Z"
    }
   },
   "outputs": [],
   "source": [
    "j = nltk.pos_tag(text)\n",
    "\n",
    "remove_parts = set(['PRP','PRP$','NNP','NNPS','EX','PDT','UH','WP','WPS','WRB'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:31:16.763492Z",
     "start_time": "2020-02-05T16:30:18.323454Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(items) for items in words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:44:58.686844Z",
     "start_time": "2020-02-05T16:44:58.657071Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def remove_parts_of_speech(tokenized_corpus, parts_to_remove):\n",
    "    '''function which uses nltk position tagging to remove parts of speach\n",
    "    ++++++++++\n",
    "    Attributes\n",
    "    tokenized_corpus (lst): a list of lists: the corpus of documents, each doc transformed into a list of tokens\n",
    "    parts_to_remove (lst): a list of the NLTK parts of speach that you want removed\n",
    "    \n",
    "    Returns: \n",
    "    ++++++++++\n",
    "    no_pronouns(lst): a list of lists simmilar to tokenized_corpus containing none of the parts of speech you wanted removed\n",
    "    '''\n",
    "    remove = set(parts_to_remove)\n",
    "    no_pronouns = []\n",
    "    for text in tokenized_corpus:\n",
    "        j =nltk.pos_tag(text)\n",
    "        review = []\n",
    "        for pos in j:\n",
    "            if pos[1] in remove:\n",
    "                continue\n",
    "            else:\n",
    "                review.append(pos[0])\n",
    "        no_pronouns.append(review)\n",
    "    \n",
    "    return no_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-05T16:45:02.773Z"
    }
   },
   "outputs": [],
   "source": [
    "without_pronouns_words = remove_parts_of_speech(tokenized_corpus, remove_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:23:09.666560Z",
     "start_time": "2020-02-05T16:23:09.570468Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', ','), 381867),\n",
       " (('the', 'DT'), 361935),\n",
       " (('.', '.'), 344774),\n",
       " (('and', 'CC'), 226619),\n",
       " (('of', 'IN'), 211056),\n",
       " (('to', 'TO'), 192696),\n",
       " (('a', 'DT'), 186677),\n",
       " (('is', 'VBZ'), 118230),\n",
       " (('in', 'IN'), 115326),\n",
       " (('was', 'VBD'), 77819),\n",
       " (('this', 'DT'), 74922),\n",
       " (('book', 'NN'), 72787),\n",
       " (('that', 'IN'), 58600),\n",
       " (('for', 'IN'), 56996),\n",
       " (('with', 'IN'), 52984),\n",
       " (('but', 'CC'), 47099),\n",
       " (('The', 'DT'), 41939),\n",
       " (('as', 'IN'), 40057),\n",
       " (('not', 'RB'), 39439),\n",
       " ((\"n't\", 'RB'), 36967),\n",
       " (('on', 'IN'), 36914),\n",
       " (('are', 'VBP'), 34423),\n",
       " (('be', 'VB'), 32801),\n",
       " ((\"'s\", 'POS'), 32310),\n",
       " ((')', ')'), 30817),\n",
       " (('about', 'IN'), 30164),\n",
       " (('(', '('), 29289),\n",
       " (('at', 'IN'), 28440),\n",
       " ((\"'s\", 'VBZ'), 26897),\n",
       " (('story', 'NN'), 25775),\n",
       " (('so', 'RB'), 25733),\n",
       " (('one', 'CD'), 25683),\n",
       " (('that', 'WDT'), 25642),\n",
       " (('from', 'IN'), 25159),\n",
       " (('an', 'DT'), 24927),\n",
       " (('by', 'IN'), 24284),\n",
       " (('!', '.'), 24175),\n",
       " (('or', 'CC'), 23165),\n",
       " (('``', '``'), 21865),\n",
       " (('all', 'DT'), 21601),\n",
       " (('just', 'RB'), 21450),\n",
       " (('?', '.'), 21107),\n",
       " (('like', 'IN'), 20909),\n",
       " (('have', 'VBP'), 20081),\n",
       " ((\"''\", \"''\"), 19517),\n",
       " (('time', 'NN'), 18986),\n",
       " (('had', 'VBD'), 18820),\n",
       " (('has', 'VBZ'), 18539),\n",
       " (('would', 'MD'), 18473),\n",
       " (('that', 'DT'), 18448),\n",
       " ((':', ':'), 18004),\n",
       " (('can', 'MD'), 17843),\n",
       " (('This', 'DT'), 17696),\n",
       " (('did', 'VBD'), 17356),\n",
       " (('characters', 'NNS'), 16934),\n",
       " (('really', 'RB'), 15923),\n",
       " (('some', 'DT'), 15308),\n",
       " (('were', 'VBD'), 15211),\n",
       " (('because', 'IN'), 15194),\n",
       " (('life', 'NN'), 15096),\n",
       " (('have', 'VB'), 14960),\n",
       " (('which', 'WDT'), 14751),\n",
       " (('up', 'RP'), 14728),\n",
       " (('read', 'VB'), 14659),\n",
       " (('very', 'RB'), 14454),\n",
       " (('if', 'IN'), 14118),\n",
       " (('into', 'IN'), 13866),\n",
       " (('...', ':'), 13861),\n",
       " (('do', 'VBP'), 13326),\n",
       " (('books', 'NNS'), 13265),\n",
       " (('could', 'MD'), 13109),\n",
       " ((';', ':'), 13071),\n",
       " (('people', 'NNS'), 12963),\n",
       " (('-', ':'), 12862),\n",
       " (('way', 'NN'), 12787),\n",
       " (('good', 'JJ'), 12532),\n",
       " (('will', 'MD'), 12417),\n",
       " (('even', 'RB'), 12365),\n",
       " (('than', 'IN'), 12277),\n",
       " (('out', 'RP'), 11210),\n",
       " (('many', 'JJ'), 11164),\n",
       " (('been', 'VBN'), 11131),\n",
       " (('other', 'JJ'), 11105),\n",
       " (('through', 'IN'), 11065),\n",
       " (('novel', 'NN'), 10880),\n",
       " (('first', 'JJ'), 10748),\n",
       " (('more', 'RBR'), 10710),\n",
       " (('no', 'DT'), 10522),\n",
       " (('reading', 'VBG'), 10447),\n",
       " (('much', 'JJ'), 10445),\n",
       " (('And', 'CC'), 10375),\n",
       " (('more', 'JJR'), 10326),\n",
       " (('also', 'RB'), 10219),\n",
       " (('But', 'CC'), 10199),\n",
       " (('does', 'VBZ'), 9674),\n",
       " (('character', 'NN'), 9480),\n",
       " (('world', 'NN'), 9461),\n",
       " ((\"'m\", 'VBP'), 9300),\n",
       " (('too', 'RB'), 9271),\n",
       " (('family', 'NN'), 9035)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.most_common(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:44:22.449401Z",
     "start_time": "2020-02-05T16:44:22.443032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having',\n",
       " 'lived',\n",
       " 'in',\n",
       " 'most',\n",
       " 'of',\n",
       " 'life',\n",
       " ',',\n",
       " 'can',\n",
       " 'say',\n",
       " 'that',\n",
       " 'found',\n",
       " \"'s\",\n",
       " 'accent',\n",
       " 'in',\n",
       " 'this',\n",
       " 'audiobook',\n",
       " 'was',\n",
       " 'right',\n",
       " 'on',\n",
       " 'the',\n",
       " 'money',\n",
       " '.',\n",
       " 'just',\n",
       " 'sounded',\n",
       " 'like',\n",
       " 'a',\n",
       " 'rural',\n",
       " 'farmer',\n",
       " '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_pronouns_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:43:28.741825Z",
     "start_time": "2020-02-05T16:43:28.711208Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-93b14f9e972d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidfed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithout_pronouns_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \"\"\"\n\u001b[1;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                tokenize)\n\u001b[1;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 352\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(stop_words ='english', lowercase = True, max_features = 10000)\n",
    "tfidfed = tfid.fit_transform(without_pronouns_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv4689841a946143dd80c9fcc86c644564"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
